{
    "summary": "The code creates a ChatGPT class that inherits from AbstractLanguageModel, initializes with configuration and model details, sets query parameters, supports multiple responses, uses OpenAI's chat API, incorporates backoff and caching for optimization, logs response texts and costs, and utilizes `get_response_texts` to extract response strings.",
    "details": [
        {
            "comment": "This code is the initialization of a class called ChatGPT. It inherits from AbstractLanguageModel and initializes with configuration, model details, and caching options. The config_path parameter is for the path to a configuration file and defaults to an empty string. The model_name parameter specifies the model to be used, defaulting to \"chatgpt\", and cache can be set to True or False for enabling or disabling caching respectively.",
            "location": "\"/media/root/Toshiba XG3/works/graph-of-thoughts/docs/src/graph_of_thoughts/language_models/chatgpt.py\":0-34",
            "content": "# Copyright (c) 2023 ETH Zurich.\n#                    All rights reserved.\n#\n# Use of this source code is governed by a BSD-style license that can be\n# found in the LICENSE file.\n#\n# main author: Nils Blach\nimport backoff\nimport os\nimport random\nimport time\nfrom typing import List, Dict, Union\nfrom openai import OpenAI, OpenAIError\nfrom openai.types.chat.chat_completion import ChatCompletion\nfrom .abstract_language_model import AbstractLanguageModel\nclass ChatGPT(AbstractLanguageModel):\n    \"\"\"\n    The ChatGPT class handles interactions with the OpenAI models using the provided configuration.\n    Inherits from the AbstractLanguageModel and implements its abstract methods.\n    \"\"\"\n    def __init__(\n        self, config_path: str = \"\", model_name: str = \"chatgpt\", cache: bool = False\n    ) -> None:\n        \"\"\"\n        Initialize the ChatGPT instance with configuration, model details, and caching options.\n        :param config_path: Path to the configuration file. Defaults to \"\".\n        :type config_path: str\n    "
        },
        {
            "comment": "The code initializes a model with a specified name and sets the cache flag. It retrieves the model ID, prompt token cost, response token cost, temperature, and maximum number of tokens for chat completion from the configuration file.",
            "location": "\"/media/root/Toshiba XG3/works/graph-of-thoughts/docs/src/graph_of_thoughts/language_models/chatgpt.py\":34-48",
            "content": "    :param model_name: Name of the model, default is 'chatgpt'. Used to select the correct configuration.\n        :type model_name: str\n        :param cache: Flag to determine whether to cache responses. Defaults to False.\n        :type cache: bool\n        \"\"\"\n        super().__init__(config_path, model_name, cache)\n        self.config: Dict = self.config[model_name]\n        # The model_id is the id of the model that is used for chatgpt, i.e. gpt-4, gpt-3.5-turbo, etc.\n        self.model_id: str = self.config[\"model_id\"]\n        # The prompt_token_cost and response_token_cost are the costs for 1000 prompt tokens and 1000 response tokens respectively.\n        self.prompt_token_cost: float = self.config[\"prompt_token_cost\"]\n        self.response_token_cost: float = self.config[\"response_token_cost\"]\n        # The temperature of a model is defined as the randomness of the model's output.\n        self.temperature: float = self.config[\"temperature\"]\n        # The maximum number of tokens to generate in the chat completion."
        },
        {
            "comment": "This code initializes an instance of a language model and sets parameters such as maximum tokens, stop sequence, organization, API key, and initializes the OpenAI client. It also includes a query method to ask the language model for responses.",
            "location": "\"/media/root/Toshiba XG3/works/graph-of-thoughts/docs/src/graph_of_thoughts/language_models/chatgpt.py\":49-68",
            "content": "        self.max_tokens: int = self.config[\"max_tokens\"]\n        # The stop sequence is a sequence of tokens that the model will stop generating at (it will not generate the stop sequence).\n        self.stop: Union[str, List[str]] = self.config[\"stop\"]\n        # The account organization is the organization that is used for chatgpt.\n        self.organization: str = self.config[\"organization\"]\n        if self.organization == \"\":\n            self.logger.warning(\"OPENAI_ORGANIZATION is not set\")\n        self.api_key: str = os.getenv(\"OPENAI_API_KEY\", self.config[\"api_key\"])\n        if self.api_key == \"\":\n            raise ValueError(\"OPENAI_API_KEY is not set\")\n        # Initialize the OpenAI Client\n        self.client = OpenAI(api_key=self.api_key, organization=self.organization)\n    def query(\n        self, query: str, num_responses: int = 1\n    ) -> Union[List[ChatCompletion], ChatCompletion]:\n        \"\"\"\n        Query the OpenAI model for responses.\n        :param query: The query to be posed to the language model."
        },
        {
            "comment": "The code defines a function that takes a query and the number of desired responses. If the query is in the cache, it returns the corresponding response(s). If not, it calls the OpenAI chat model to generate responses for the given query. It supports generating multiple responses by repeatedly calling the OpenAI model until the required number of responses are obtained or an exception occurs. The function also logs any warnings during the process.",
            "location": "\"/media/root/Toshiba XG3/works/graph-of-thoughts/docs/src/graph_of_thoughts/language_models/chatgpt.py\":69-93",
            "content": "        :type query: str\n        :param num_responses: Number of desired responses, default is 1.\n        :type num_responses: int\n        :return: Response(s) from the OpenAI model.\n        :rtype: Dict\n        \"\"\"\n        if self.cache and query in self.respone_cache:\n            return self.respone_cache[query]\n        if num_responses == 1:\n            response = self.chat([{\"role\": \"user\", \"content\": query}], num_responses)\n        else:\n            response = []\n            next_try = num_responses\n            total_num_attempts = num_responses\n            while num_responses > 0 and total_num_attempts > 0:\n                try:\n                    assert next_try > 0\n                    res = self.chat([{\"role\": \"user\", \"content\": query}], next_try)\n                    response.append(res)\n                    num_responses -= next_try\n                    next_try = min(num_responses, next_try)\n                except Exception as e:\n                    next_try = (next_try + 1) // 2\n                    self.logger.warning("
        },
        {
            "comment": "This code is defining a class with a chat method that sends messages to the OpenAI model and retrieves the response. The method implements backoff on OpenAI error, allowing for multiple attempts if an error occurs. It also includes caching functionality to improve performance by storing previous responses in a cache.",
            "location": "\"/media/root/Toshiba XG3/works/graph-of-thoughts/docs/src/graph_of_thoughts/language_models/chatgpt.py\":94-118",
            "content": "                        f\"Error in chatgpt: {e}, trying again with {next_try} samples\"\n                    )\n                    time.sleep(random.randint(1, 3))\n                    total_num_attempts -= 1\n        if self.cache:\n            self.respone_cache[query] = response\n        return response\n    @backoff.on_exception(backoff.expo, OpenAIError, max_time=10, max_tries=6)\n    def chat(self, messages: List[Dict], num_responses: int = 1) -> ChatCompletion:\n        \"\"\"\n        Send chat messages to the OpenAI model and retrieves the model's response.\n        Implements backoff on OpenAI error.\n        :param messages: A list of message dictionaries for the chat.\n        :type messages: List[Dict]\n        :param num_responses: Number of desired responses, default is 1.\n        :type num_responses: int\n        :return: The OpenAI model's response.\n        :rtype: ChatCompletion\n        \"\"\"\n        response = self.client.chat.completions.create(\n            model=self.model_id,\n            messages=messages,"
        },
        {
            "comment": "This code interacts with an OpenAI model, specifically the ChatGPT API. It takes a query as input and generates multiple responses using the API. The code keeps track of usage costs in terms of prompt and completion tokens, and logs the response text along with the cost for each generated response. The `get_response_texts` method extracts the response texts from the query response dictionary or list of dictionaries returned by the OpenAI model.",
            "location": "\"/media/root/Toshiba XG3/works/graph-of-thoughts/docs/src/graph_of_thoughts/language_models/chatgpt.py\":119-145",
            "content": "            temperature=self.temperature,\n            max_tokens=self.max_tokens,\n            n=num_responses,\n            stop=self.stop,\n        )\n        self.prompt_tokens += response.usage.prompt_tokens\n        self.completion_tokens += response.usage.completion_tokens\n        prompt_tokens_k = float(self.prompt_tokens) / 1000.0\n        completion_tokens_k = float(self.completion_tokens) / 1000.0\n        self.cost = (\n            self.prompt_token_cost * prompt_tokens_k\n            + self.response_token_cost * completion_tokens_k\n        )\n        self.logger.info(\n            f\"This is the response from chatgpt: {response}\"\n            f\"\\nThis is the cost of the response: {self.cost}\"\n        )\n        return response\n    def get_response_texts(\n        self, query_response: Union[List[ChatCompletion], ChatCompletion]\n    ) -> List[str]:\n        \"\"\"\n        Extract the response texts from the query response.\n        :param query_response: The response dictionary (or list of dictionaries) from the OpenAI model."
        },
        {
            "comment": "This function converts a single ChatCompletion or list of them into a list of response strings by iterating over the choices within each completion and extracting their content.",
            "location": "\"/media/root/Toshiba XG3/works/graph-of-thoughts/docs/src/graph_of_thoughts/language_models/chatgpt.py\":146-156",
            "content": "        :type query_response: Union[List[ChatCompletion], ChatCompletion]\n        :return: List of response strings.\n        :rtype: List[str]\n        \"\"\"\n        if not isinstance(query_response, List):\n            query_response = [query_response]\n        return [\n            choice.message.content\n            for response in query_response\n            for choice in response.choices\n        ]"
        }
    ]
}