{
    "summary": "The code develops an efficient NDA merging class with language model prompts and redundancy handling, generating a graph for document merge and language model inference within budget limits. It utilizes input data from \"documents.csv\", manages exceptions, and scores output based on coverage.",
    "details": [
        {
            "comment": "This code defines a class DocMergePrompter that inherits from Prompter and provides prompts for merging NDA documents. It includes a merge_doc_prompt_start string for generating the prompt and a merge_doc_prompt_block string for displaying NDAs to be merged. The goal is to create a single NDA by maximizing information retention and minimizing redundancy, with the output between <Merged> and </Merged>.",
            "location": "\"/media/root/Toshiba XG3/works/graph-of-thoughts/docs/src/examples/doc_merge/doc_merge.py\":0-30",
            "content": "# Copyright (c) 2023 ETH Zurich.\n#                    All rights reserved.\n#\n# Use of this source code is governed by a BSD-style license that can be\n# found in the LICENSE file.\n#\n# main author: Nils Blach\nimport os\nimport re\nimport logging\nimport datetime\nimport json\nimport csv\nfrom statistics import fmean\nfrom typing import Dict, List, Callable, Set, Union\nfrom graph_of_thoughts import controller, language_models, operations, prompter, parser\nclass DocMergePrompter(prompter.Prompter):\n    \"\"\"\n    DocMergePrompter provides the generation of prompts specific to the document\n    merge example for the language models.\n    Inherits from the Prompter class and implements its abstract methods.\n    \"\"\"\n    merge_doc_prompt_start = \"\"\"Merge the following {num} NDA documents <Doc1> - <Doc{num}> into a single NDA, maximizing retained information and minimizing redundancy. Output only the created NDA between the tags <Merged> and </Merged>, without any additional text.\nHere are NDAs <Doc1> - <Doc{num}>\n\"\"\"\n    merge_doc_prompt_block = \"\"\""
        },
        {
            "comment": "The code defines two prompts for merging and improving NDA documents. The first prompt instructs to merge the provided NDAs into a single one, preserving information and minimizing redundancy. It also provides an example approach. The second prompt asks to improve the merged document by adding more information and removing redundancies, with output placed between specific tags. Both prompts include the input NDAs as \"Doc1\" to \"Doc{num}\".",
            "location": "\"/media/root/Toshiba XG3/works/graph-of-thoughts/docs/src/examples/doc_merge/doc_merge.py\":31-53",
            "content": "<Doc{num}>\n{document}\n</Doc{num}>\n\"\"\"\n    merge_doc_prompt_cot_start = \"\"\"Merge the following {num} NDA documents <Doc1> - <Doc{num}> into a single NDA, maximizing retained information and minimizing redundancy.\nYou can generate any intermediate thoughts and documents you want, but the final output should be the merged NDA, placed between the two tags <Merged> and </Merged>.\nFor instance you might want to follow this approach:\n1. Split each NDA into their logical subparts.\n2. Merge the subparts of the {num} NDAs.\n3. Combine the merged subparts into a single NDA.\n4. Place the merged NDA between the tags <Merged> and </Merged>.\nHere are NDAs <Doc1> - <Doc{num}>:\n\"\"\"\n    improve_summary_prompt_start = \"\"\"The following NDA <S> merges initial NDAs <Doc1> - <Doc{num}>.\nPlease improve the summary NDA <S> by adding more information and removing redundancy. Output only the improved NDA, placed between the two tags <Merged> and </Merged>, without any additional text.\nHere are NDAs <Doc1> - <Doc{num}>:\n\"\"\"\n    improve_summary_prompt_block = \"\"\""
        },
        {
            "comment": "This code contains various prompts for different tasks, such as improving summaries and scoring merged documents. The prompts are designed to assist in the task of merging NDAs while considering redundancy and retained information scores, with specific tags provided for clarity.",
            "location": "\"/media/root/Toshiba XG3/works/graph-of-thoughts/docs/src/examples/doc_merge/doc_merge.py\":54-70",
            "content": "<Doc{num}>\n{document}\n</Doc{num}>\n\"\"\"\n    improve_summary_prompt_end = \"\"\"\nHere is the summary NDA <S>:\n<S>\n{summary}\n</S>\n\"\"\"\n    score_prompt_base = \"\"\"The following NDA <S> merges NDAs <Doc1> - <Doc{num}>.\nPlease score the merged NDA <S> in terms of how much redundant information is contained, independent of the original NDAs, as well as how much information is retained from the original NDAs.\nA score of 10 for redundancy implies that absolutely no information is redundant, while a score of 0 implies that at least half of the information is redundant (so everything is at least mentioned twice).\nA score of 10 for retained information implies that all information from the original NDAs is retained, while a score of 0 implies that no information is retained.\nYou may provide reasoning for your scoring, but the final score for redundancy should be between the tags <Redundancy> and </Redundancy>, and the final score for retained information should be between the tags <Retained> and </Retained>, without any additional text within any of those tags."
        },
        {
            "comment": "This code appears to be part of a larger program that deals with merging and summarizing Non-Disclosure Agreements (NDAs). It uses string formatting to generate prompts for the user, asking them to provide NDAs in a specific format. The code snippet includes various placeholders (<Doc>, <S>) for incorporating the user's provided information.",
            "location": "\"/media/root/Toshiba XG3/works/graph-of-thoughts/docs/src/examples/doc_merge/doc_merge.py\":72-111",
            "content": "Here are NDAs <Doc1> - <Doc{num}>:\n\"\"\"\n    score_prompt_block = \"\"\"\n<Doc{num}>\n{document}\n</Doc{num}>\n\"\"\"\n    score_prompt_end = \"\"\"\nHere is the summary NDA <S>:\n<S>\n{summary}\n</S>\n\"\"\"\n    aggregate_full_prompt_base = \"\"\"The following NDAs <S1> - <S{num_ndas_summary}> each merge the initial NDAs <Doc1> - <Doc{num_ndas}>.\nCombine the merged NDAs <S1> - <S{num_ndas_summary}> into a new one, maximizing their advantages and overall information retention, while minimizing redundancy.\nOutput only the new NDA between the tags <Merged> and </Merged>, without any additional text.   \nHere are the original NDAs <Doc1> - <Doc{num_ndas}>:\n\"\"\"\n    aggregate_full_prompt_block1 = \"\"\"\n<Doc{num}>\n{document}\n</Doc{num}>\n\"\"\"\n    aggregate_full_prompt_mid = \"\"\"\nHere are the summary NDAs <S1> - <S{num_ndas_summary}>:\n\"\"\"\n    aggregate_full_prompt_block2 = \"\"\"\n<S{num}>\n{summary}\n</S{num}>\n\"\"\"\n    aggregate_sub_prompt_base = \"\"\"The following NDAs <S1> - <S{num_ndas}> are summaries of some other NDAs.\nCombine them into a new one, make sure to maximize their advantages and overall information retention, while minimizing redundancy."
        },
        {
            "comment": "This code generates an aggregation prompt for a language model, using the provided state_dicts. It concatenates NDAs from each state_dict and formats them into a final prompt. The output is a string containing the merged NDAs between \"<Merged>\" and \"</Merged>\".",
            "location": "\"/media/root/Toshiba XG3/works/graph-of-thoughts/docs/src/examples/doc_merge/doc_merge.py\":112-142",
            "content": "Output only the new NDA between the tags <Merged> and </Merged>, without any additional text.\nHere are NDAs <S1> - <S{num_ndas}>:\n\"\"\"\n    aggregate_sub_prompt_generate = \"\"\"\nNDA <S{num}>:\n{nda}\n</S{num}>\n\"\"\"\n    def aggregation_prompt(self, state_dicts: List[Dict], **kwargs) -> str:\n        \"\"\"\n        Generate an aggregation prompt for the language model.\n        :param state_dicts: The thought states that should be aggregated.\n        :type state_dicts: List[Dict]\n        :param kwargs: Additional keyword arguments.\n        :return: The aggregation prompt.\n        :rtype: str\n        \"\"\"\n        if len(state_dicts[0][\"parts\"]) > 0 and len(state_dicts[0][\"parts\"]) < len(\n            state_dicts[0][\"documents\"]\n        ):\n            prompt = self.aggregate_sub_prompt_base.format(\n                num_ndas=len(state_dicts),\n            )\n            for i, state_dict in enumerate(state_dicts):\n                prompt += self.aggregate_sub_prompt_generate.format(\n                    nda=state_dict[\"current\"], num=i + 1"
        },
        {
            "comment": "This code defines a class with methods for generating prompts. The `generate_prompt` method takes in parameters like number of branches, documents, and current state. It returns a prompt for the language model using string formatting based on input parameters.",
            "location": "\"/media/root/Toshiba XG3/works/graph-of-thoughts/docs/src/examples/doc_merge/doc_merge.py\":143-173",
            "content": "                )\n            return prompt\n        else:\n            prompt = self.aggregate_full_prompt_base.format(\n                num_ndas=len(state_dicts[0][\"documents\"]),\n                num_ndas_summary=len(state_dicts),\n            )\n            for i, document in enumerate(state_dicts[0][\"documents\"]):\n                prompt += self.aggregate_full_prompt_block1.format(\n                    document=document, num=i + 1\n                )\n            prompt += self.aggregate_full_prompt_mid.format(\n                num_ndas_summary=len(state_dicts),\n            )\n            for i, state_dict in enumerate(state_dicts):\n                prompt += self.aggregate_full_prompt_block2.format(\n                    summary=state_dict[\"current\"], num=i + 1\n                )\n            return prompt\n    def generate_prompt(\n        self,\n        num_branches: int,\n        documents: List[str],\n        method: str,\n        parts: Set[str],\n        current: str,\n        **kwargs,\n    ) -> str:\n        \"\"\"\n        Generate a generate prompt for the language model."
        },
        {
            "comment": "This function takes in the number of responses, a list of documents to merge, method for generating the prompt, indices of already processed document parts, an intermediate solution, and additional keyword arguments. It returns the generate prompt used for merging the documents. If the method is not implemented yet, it raises AssertionError.",
            "location": "\"/media/root/Toshiba XG3/works/graph-of-thoughts/docs/src/examples/doc_merge/doc_merge.py\":175-197",
            "content": "        :param num_branches: The number of responses the prompt should ask the LM to generate.\n        :type num_branches: int\n        :param documents: The list of documents to be merged.\n        :type documents: List[str]\n        :param method: Method for which the generate prompt is generated.\n        :type method: str\n        :param parts: Indices of the already processed document parts.\n        :type parts: Set[str]\n        :param current: The intermediate solution.\n        :type current: str\n        :param kwargs: Additional keyword arguments.\n        :return: The generate prompt.\n        :rtype: str\n        :raise AssertionError: If method is not implemented yet.\n        \"\"\"\n        prompt = \"\"\n        if method.startswith(\"io\") or method.startswith(\"cot\"):\n            if method.startswith(\"io\"):\n                prompt += self.merge_doc_prompt_start.format(num=len(documents))\n            else:\n                prompt += self.merge_doc_prompt_cot_start.format(num=len(documents))\n            for i, document in enumerate(documents):"
        },
        {
            "comment": "The code provides a prompt for merging multiple documents or improving a given summary based on the specified method. It dynamically generates the prompt by concatenating predefined blocks of text with placeholders for document numbers and the original summary. If no current summary is provided, it creates a prompt to merge documents, otherwise, it improves the given summary using those documents.",
            "location": "\"/media/root/Toshiba XG3/works/graph-of-thoughts/docs/src/examples/doc_merge/doc_merge.py\":198-220",
            "content": "                prompt += self.merge_doc_prompt_block.format(\n                    document=document, num=i + 1\n                )\n            return prompt\n        elif method.startswith(\"tot\"):\n            if current is None or current == \"\":\n                prompt += self.merge_doc_prompt_start.format(num=len(documents))\n                for i, document in enumerate(documents):\n                    prompt += self.merge_doc_prompt_block.format(\n                        document=document, num=i + 1\n                    )\n                return prompt\n            else:\n                prompt += self.improve_summary_prompt_start.format(\n                    num=len(documents),\n                )\n                for i, document in enumerate(documents):\n                    prompt += self.improve_summary_prompt_block.format(\n                        document=document, num=i + 1\n                    )\n                prompt += self.improve_summary_prompt_end.format(summary=current)\n                return prompt\n        elif method.startswith(\"got\"):"
        },
        {
            "comment": "The code checks if the current summary is provided. If not, it generates a prompt for merging documents into one coherent summary. If the current summary is provided, it generates a prompt for improving an existing summary by incorporating information from multiple documents. The code also sorts the parts of the document and formats them in a specific way for the prompts.",
            "location": "\"/media/root/Toshiba XG3/works/graph-of-thoughts/docs/src/examples/doc_merge/doc_merge.py\":221-244",
            "content": "            parts = (\n                sorted(list(parts)) if len(parts) > 0 else list(range(len(documents)))\n            )\n            if current is None or current == \"\":\n                prompt += self.merge_doc_prompt_start.format(num=len(parts))\n                for i, part in enumerate(sorted(list(parts))):\n                    prompt += self.merge_doc_prompt_block.format(\n                        document=documents[part], num=i + 1\n                    )\n                return prompt\n            else:\n                prompt += self.improve_summary_prompt_start.format(\n                    num=len(parts),\n                )\n                for i, part in enumerate(sorted(list(parts))):\n                    prompt += self.improve_summary_prompt_block.format(\n                        document=documents[part], num=i + 1\n                    )\n                prompt += self.improve_summary_prompt_end.format(summary=current)\n                return prompt\n        else:\n            assert False, \"Not implemented yet.\"\n    def score_prompt(self, state_dicts: List[Dict], **kwargs) -> str:"
        },
        {
            "comment": "This function generates a score prompt for the language model using a single thought state provided as an argument. It checks if only one thought state is supplied and handles the case where more than one is given. The prompt is created by formatting the base and block prompts with the number of documents.",
            "location": "\"/media/root/Toshiba XG3/works/graph-of-thoughts/docs/src/examples/doc_merge/doc_merge.py\":245-273",
            "content": "        \"\"\"\n        Generate a score prompt for the language model.\n        :param state_dicts: The thought states that should be scored,\n                            if more than one, they should be scored together.\n        :type state_dicts: List[Dict]\n        :param kwargs: Additional keyword arguments.\n        :return: The score prompt.\n        :rtype: str\n        :raise AssertionError: If more than one thought state is supplied.\n        \"\"\"\n        if len(state_dicts) > 1:\n            assert False, \"Not implemented yet.\"\n        else:\n            # perform individual scoring\n            parts = (\n                [\n                    state_dicts[0][\"documents\"][part]\n                    for part in sorted(list(state_dicts[0][\"parts\"]))\n                ]\n                if len(state_dicts[0][\"parts\"]) > 0\n                else state_dicts[0][\"documents\"]\n            )\n            prompt = self.score_prompt_base.format(\n                num=len(parts),\n            )\n            for i, part in enumerate(parts):\n                prompt += self.score_prompt_block.format(document=part, num=i + 1)"
        },
        {
            "comment": "This code defines a class DocMergeParser that extends the Parser class and provides specific functionality for parsing language model responses in the document merge example. It includes methods to generate improve prompt, validation prompt, and handles answer stripping with optional tags. The response cache is initialized in the constructor.",
            "location": "\"/media/root/Toshiba XG3/works/graph-of-thoughts/docs/src/examples/doc_merge/doc_merge.py\":274-314",
            "content": "            prompt += self.score_prompt_end.format(\n                summary=state_dicts[0][\"current\"],\n            )\n            return prompt\n    def improve_prompt(self, **kwargs) -> str:\n        \"\"\"\n        Generate an improve prompt for the language model.\n        :param kwargs: Additional keyword arguments.\n        :return: The improve prompt.\n        :rtype: str\n        \"\"\"\n        pass\n    def validation_prompt(self, **kwargs) -> str:\n        \"\"\"\n        Generate a validation prompt for the language model.\n        :param kwargs: Additional keyword arguments.\n        :return: The validation prompt.\n        :rtype: str\n        \"\"\"\n        pass\nclass DocMergeParser(parser.Parser):\n    \"\"\"\n    DocMergeParser provides the parsing of language model reponses specific to the\n    document merge example.\n    Inherits from the Parser class and implements its abstract methods.\n    \"\"\"\n    def __init__(self) -> None:\n        \"\"\"\n        Inits the response cache.\n        \"\"\"\n        self.cache = {}\n    def strip_answer_helper(self, text: str, tag: str = \"\") -> str:"
        },
        {
            "comment": "This function removes specified tags from a text. It first strips whitespace and checks if \"Output:\" is in the text. Then, it searches for start and end tags to remove the enclosed content while handling cases of only one tag found. If no matching tags are found, it logs a warning and returns everything after or before the found tag.",
            "location": "\"/media/root/Toshiba XG3/works/graph-of-thoughts/docs/src/examples/doc_merge/doc_merge.py\":315-341",
            "content": "        \"\"\"\n        Helper function to remove tags from a text.\n        :param text: The input text.\n        :type text: str\n        :param tag: The tag to be stripped. Defaults to \"\".\n        :type tag: str\n        :return: The stripped text.\n        :rtype: str\n        \"\"\"\n        text = text.strip()\n        if \"Output:\" in text:\n            text = text[text.index(\"Output:\") + len(\"Output:\") :].strip()\n        if tag != \"\":\n            start = text.rfind(f\"<{tag}>\")\n            end = text.rfind(f\"</{tag}>\")\n            if start != -1 and end != -1:\n                text = text[start + len(f\"<{tag}>\") : end].strip()\n            elif start != -1:\n                logging.warning(\n                    f\"Only found the start tag <{tag}> in answer: {text}. Returning everything after the tag.\"\n                )\n                text = text[start + len(f\"<{tag}>\") :].strip()\n            elif end != -1:\n                logging.warning(\n                    f\"Only found the end tag </{tag}> in answer: {text}. Returning everything before the tag.\""
        },
        {
            "comment": "The code is parsing the response from a language model for an aggregation prompt. It checks if there are enough thought states and performs subpart aggregation by stripping the answer to a single text using a helper function.",
            "location": "\"/media/root/Toshiba XG3/works/graph-of-thoughts/docs/src/examples/doc_merge/doc_merge.py\":342-368",
            "content": "                )\n                text = text[:end].strip()\n            else:\n                logging.warning(\n                    f\"Could not find any tag {tag} in answer: {text}. Returning the full answer.\"\n                )\n        return text\n    def parse_aggregation_answer(\n        self, states: List[Dict], texts: List[str]\n    ) -> Union[Dict, List[Dict]]:\n        \"\"\"\n        Parse the response from the language model for an aggregation prompt.\n        :param states: The thought states used to generate the prompt.\n        :type states: List[Dict]\n        :param texts: The responses to the prompt from the language model.\n        :type texts: List[str]\n        :return: The new thought states after parsing the respones from the language model.\n        :rtype: Union[Dict, List[Dict]]\n        \"\"\"\n        new_states = []\n        for text in texts:\n            if len(states[0][\"parts\"]) < len(states[0][\"documents\"]):\n                # subpart aggregation\n                text = self.strip_answer_helper(text, \"Merged\")"
        },
        {
            "comment": "The code appears to be a part of a larger function that generates new thought states by aggregating inputs from multiple sources. It seems to handle both partial and full non-disclosure agreement (NDA) cases, stripping the answer text and creating new states accordingly. The `parse_generate_answer` function processes response from the language model for generate prompts and returns new thought states after parsing the responses.",
            "location": "\"/media/root/Toshiba XG3/works/graph-of-thoughts/docs/src/examples/doc_merge/doc_merge.py\":369-392",
            "content": "                new_state = states[0].copy()\n                new_state[\"current\"] = text\n                new_state[\"parts\"] = set()\n                for state in states:\n                    new_state[\"parts\"] = new_state[\"parts\"] | state[\"parts\"]\n                new_states.append(new_state)\n            else:\n                # full NDA aggregation\n                text = self.strip_answer_helper(text, \"Merged\")\n                new_state = states[0].copy()\n                new_state[\"current\"] = text\n                new_states.append(new_state)\n        return new_states\n    def parse_generate_answer(self, state: Dict, texts: List[str]) -> List[Dict]:\n        \"\"\"\n        Parse the response from the language model for a generate prompt.\n        :param state: The thought state used to generate the prompt.\n        :type state: Dict\n        :param texts: The responses to the prompt from the language model.\n        :type texts: List[str]\n        :return: The new thought states after parsing the respones from the language model."
        },
        {
            "comment": "The function `get_new_states()` takes a list of texts and returns a list of dictionaries, where each dictionary represents a thought state with the current text as its value.\n\nThe function `parse_score_answer()` takes a list of thought states and responses from the language model, asserts that only one thought state is allowed for scoring, and then initializes lists for redundancy and retain scores.",
            "location": "\"/media/root/Toshiba XG3/works/graph-of-thoughts/docs/src/examples/doc_merge/doc_merge.py\":393-419",
            "content": "        :rtype: List[Dict]\n        \"\"\"\n        new_states = []\n        for text in texts:\n            text = self.strip_answer_helper(text, \"Merged\")\n            new_state = state.copy()\n            new_state[\"current\"] = text\n            new_states.append(new_state)\n        return new_states\n    def parse_score_answer(self, states: List[Dict], texts: List[str]) -> List[float]:\n        \"\"\"\n        Parse the response from the language model for a score prompt.\n        :param states: The thought states used to generate the prompt.\n        :type states: List[Dict]\n        :param texts: The responses to the prompt from the language model.\n        :type texts: List[str]\n        :return: The scores for the thought states.\n        :rtype: List[float]\n        :raise AssertionError: If the number of thought states is not one.\n        \"\"\"\n        assert len(states) == 1, \"Only one state is allowed for scoring.\"\n        if len(states) == 1:\n            # individual scoring\n            redundancy_scores = []\n            retain_scores = []"
        },
        {
            "comment": "This code iterates through text inputs, extracts redundancy and retained scores using regex, handles multiple score cases by logging a warning and selecting the last one or ignoring if no scores found.",
            "location": "\"/media/root/Toshiba XG3/works/graph-of-thoughts/docs/src/examples/doc_merge/doc_merge.py\":420-440",
            "content": "            for text in texts:\n                answer = self.strip_answer_helper(text, \"Redundancy\")\n                res = re.findall(r\"\\d+\\.?\\d*\", answer)\n                if len(res) == 1:\n                    redundancy_scores.append(float(res[0]))\n                elif len(res) > 1:\n                    logging.warning(\n                        f\"Found multiple redundancy scores in answer: {text}. Returning the last one.\"\n                    )\n                    redundancy_scores.append(float(res[-1]))\n                else:\n                    logging.warning(\n                        f\"Could not find any redundancy score in answer: {text}. Ignoring this answer.\"\n                    )\n                answer = self.strip_answer_helper(text, \"Retained\")\n                res = re.findall(r\"\\d+\\.?\\d*\", answer)\n                if len(res) == 1:\n                    retain_scores.append(float(res[0]))\n                elif len(res) > 1:\n                    logging.warning(\n                        f\"Found multiple retained scores in answer: {text}. Returning the last one.\""
        },
        {
            "comment": "This code snippet is a part of a function responsible for parsing the responses from a language model for an 'improve' prompt. It calculates redundancy and retain scores for each answer, then returns the F1 score based on these scores. If no valid scores are found in any answer, it returns 0.0.",
            "location": "\"/media/root/Toshiba XG3/works/graph-of-thoughts/docs/src/examples/doc_merge/doc_merge.py\":441-463",
            "content": "                    )\n                    retain_scores.append(float(res[-1]))\n                else:\n                    logging.warning(\n                        f\"Could not find any retained score in answer: {text}. Ignoring this answer.\"\n                    )\n            if len(redundancy_scores) == 0 or len(retain_scores) == 0:\n                logging.warning(\n                    f\"Could not find any valid score in any answer. Returning 0.0.\"\n                )\n                return [0.0]\n            mean_redundancy = fmean(redundancy_scores)\n            mean_retain = fmean(retain_scores)\n            f1 = 2 * mean_redundancy * mean_retain / (mean_redundancy + mean_retain)\n            return [f1]\n    def parse_improve_answer(self, state: Dict, texts: List[str]) -> Dict:\n        \"\"\"\n        Parse the response from the language model for an improve prompt.\n        :param state: The thought state used to generate the prompt.\n        :type state: Dict\n        :param texts: The responses to the prompt from the language model."
        },
        {
            "comment": "This code contains functions for thought state management, parsing responses from a language model, and generating the Graph of Operations for IO method. It uses Dict and List[str] as inputs and returns bool or Dict outputs. The code block defines three functions: update_thought_state, parse_validation_answer, and io. The last function generates the Graph of Operations by appending Generate and Score operations to an instance of operations.GraphOfOperations().",
            "location": "\"/media/root/Toshiba XG3/works/graph-of-thoughts/docs/src/examples/doc_merge/doc_merge.py\":464-496",
            "content": "        :type texts: List[str]\n        :return: The new thought state after parsing the responses from the language model.\n        :rtype: Dict\n        \"\"\"\n        pass\n    def parse_validation_answer(self, state: Dict, texts: List[str]) -> bool:\n        \"\"\"\n        Parse the response from the language model for a validation prompt.\n        :param state: The thought state used to generate the prompt.\n        :type state: Dict\n        :param texts: The responses to the prompt from the language model.\n        :type texts: List[str]\n        :return: Whether the thought state is valid or not.\n        :rtype: bool\n        \"\"\"\n        pass\ndef io() -> operations.GraphOfOperations:\n    \"\"\"\n    Generates the Graph of Operations for the IO method.\n    :return: Graph of Operations\n    :rtype: GraphOfOperations\n    \"\"\"\n    operations_graph = operations.GraphOfOperations()\n    operations_graph.append_operation(operations.Generate(1, 1))\n    operations_graph.append_operation(operations.Score(3, False))\n    return operations_graph"
        },
        {
            "comment": "The code defines two functions, `cot()` and `tot()`, which generate the Graph of Operations for CoT and ToT methods respectively. The CoT method involves generating one child node, scoring it, while the ToT method generates 10 children nodes initially, keeps the best one, then generates two additional children per iteration.",
            "location": "\"/media/root/Toshiba XG3/works/graph-of-thoughts/docs/src/examples/doc_merge/doc_merge.py\":499-532",
            "content": "def cot() -> operations.GraphOfOperations:\n    \"\"\"\n    Generates the Graph of Operations for the CoT method.\n    :return: Graph of Operations\n    :rtype: GraphOfOperations\n    \"\"\"\n    operations_graph = operations.GraphOfOperations()\n    operations_graph.append_operation(operations.Generate(1, 1))\n    operations_graph.append_operation(operations.Score(3, False))\n    return operations_graph\ndef tot() -> operations.GraphOfOperations:\n    \"\"\"\n    Generates the Graph of Operations for the ToT method.\n    :return: Graph of Operations\n    :rtype: GraphOfOperations\n    \"\"\"\n    operations_graph = operations.GraphOfOperations()\n    branch_factor = 10\n    operations_graph.append_operation(operations.Generate(1, branch_factor))\n    operations_graph.append_operation(operations.Score(3, False))\n    keep_best_1 = operations.KeepBestN(1, True)\n    operations_graph.append_operation(keep_best_1)\n    for _ in range(2):\n        operations_graph.append_operation(operations.Generate(1, branch_factor))\n        operations_graph.append_operation(operations.Score(3, False))"
        },
        {
            "comment": "This code generates a Graph of Operations for merging full documents. It first appends operations to generate, score, aggregate, and keep the best scores. The last two operations add a predecessor to keep_best and append an additional generate operation with parameters 1 and 10.",
            "location": "\"/media/root/Toshiba XG3/works/graph-of-thoughts/docs/src/examples/doc_merge/doc_merge.py\":533-560",
            "content": "        keep_best_2 = operations.KeepBestN(1, True)\n        keep_best_2.add_predecessor(keep_best_1)\n        operations_graph.append_operation(keep_best_2)\n        keep_best_1 = keep_best_2\n    return operations_graph\ndef got() -> operations.GraphOfOperations:\n    \"\"\"\n    Generates the Graph of Operations for the GoT method, where full documents\n    are merged.\n    :return: Graph of Operations\n    :rtype: GraphOfOperations\n    \"\"\"\n    operations_graph = operations.GraphOfOperations()\n    operations_graph.append_operation(operations.Generate(1, 5))\n    operations_graph.append_operation(operations.Score(3, False))\n    keep_best = operations.KeepBestN(3, True)\n    operations_graph.append_operation(keep_best)\n    operations_graph.append_operation(operations.Aggregate(5))\n    operations_graph.append_operation(operations.Score(3, False))\n    keep_best2 = operations.KeepBestN(1, True)\n    keep_best2.add_predecessor(keep_best)\n    operations_graph.append_operation(keep_best2)\n    operations_graph.append_operation(operations.Generate(1, 10))"
        },
        {
            "comment": "This code generates a Graph of Operations for the GoT2 method, which merges partial documents. It creates an initial GraphOfOperations object and iteratively adds operations such as Selectors, Generators, and Scorers to the graph. Each iteration consists of selecting specific thoughts, generating new documents, and scoring them. The resulting graph is returned.",
            "location": "\"/media/root/Toshiba XG3/works/graph-of-thoughts/docs/src/examples/doc_merge/doc_merge.py\":561-592",
            "content": "    operations_graph.append_operation(operations.Score(3, False))\n    keep_best3 = operations.KeepBestN(1, True)\n    keep_best3.add_predecessor(keep_best2)\n    operations_graph.append_operation(keep_best3)\n    return operations_graph\ndef got2() -> operations.GraphOfOperations:\n    \"\"\"\n    Generates the Graph of Operations for the GoT2 method, where partial\n    documents are merged.\n    :return: Graph of Operations\n    :rtype: GraphOfOperations\n    \"\"\"\n    operations_graph = operations.GraphOfOperations()\n    sub_parts = []\n    for i in range(0, 4, 2):  # should be at most 16 parts\n        sub_text = operations.Selector(\n            lambda thoughts, list_id=i: [\n                operations.Thought(\n                    state={**thoughts[0].state, \"parts\": {list_id, list_id + 1}}\n                )\n            ]\n        )\n        operations_graph.add_operation(sub_text)\n        gen_nda = operations.Generate(1, 5)\n        gen_nda.add_predecessor(sub_text)\n        operations_graph.add_operation(gen_nda)\n        score_nda = operations.Score(3, False)"
        },
        {
            "comment": "This code is creating an operations graph for a document merge process. It starts with adding Score and Generate nodes, then iteratively adds Aggregate, Score, and KeepBestN nodes until there's only one node left in the sub_parts list. The Score nodes are used to calculate similarity scores, while the KeepBestN nodes keep the best result from the previous operation. The operations graph is then built with these operations added in sequence.",
            "location": "\"/media/root/Toshiba XG3/works/graph-of-thoughts/docs/src/examples/doc_merge/doc_merge.py\":593-618",
            "content": "        score_nda.add_predecessor(gen_nda)\n        operations_graph.add_operation(score_nda)\n        keep_best_nda = operations.KeepBestN(1, True)\n        keep_best_nda.add_predecessor(score_nda)\n        operations_graph.add_operation(keep_best_nda)\n        sub_parts.append(keep_best_nda)\n    while len(sub_parts) > 1:\n        new_sub_parts = []\n        for i in range(0, len(sub_parts), 2):\n            if i + 1 == len(sub_parts):\n                new_sub_parts.append(sub_parts[i])\n                continue\n            aggregate = operations.Aggregate(5)\n            aggregate.add_predecessor(sub_parts[i])\n            aggregate.add_predecessor(sub_parts[i + 1])\n            operations_graph.add_operation(aggregate)\n            score = operations.Score(3, False)\n            score.add_predecessor(aggregate)\n            operations_graph.add_operation(score)\n            keep_best = operations.KeepBestN(1, True)\n            keep_best.add_predecessor(score)\n            operations_graph.add_operation(keep_best)\n            gen_nda = operations.Generate(1, 5)"
        },
        {
            "comment": "This code is creating a graph of operations for language model inference. It defines several nodes and adds them to the operations graph, including generation, scoring, and keeping the best node. The function run() executes methods for each specified sample within the budget limit.",
            "location": "\"/media/root/Toshiba XG3/works/graph-of-thoughts/docs/src/examples/doc_merge/doc_merge.py\":619-648",
            "content": "            gen_nda.add_predecessor(keep_best)\n            operations_graph.add_operation(gen_nda)\n            score_nda = operations.Score(3, False)\n            score_nda.add_predecessor(gen_nda)\n            operations_graph.add_operation(score_nda)\n            keep_best_nda = operations.KeepBestN(1, True)\n            keep_best_nda.add_predecessor(score_nda)\n            keep_best_nda.add_predecessor(keep_best)\n            operations_graph.add_operation(keep_best_nda)\n            new_sub_parts.append(keep_best_nda)\n        sub_parts = new_sub_parts\n    return operations_graph\ndef run(\n    data_ids: List[int],\n    methods: List[Callable[[], operations.GraphOfOperations]],\n    budget: float,\n    lm_name: str,\n) -> float:\n    \"\"\"\n    Controller function that executes each specified method for each specified\n    sample while the budget is not exhausted.\n    :param data_ids: Indices of the sample to be run.\n    :type data_ids: List[int]\n    :param methods: List of functions to generate Graphs of Operations.\n    :type methods: Each function generates a Graph of Operation."
        },
        {
            "comment": "This function takes a budget, language model name, and optional data IDs as input. It reads the \"documents.csv\" file, filters the data based on provided data IDs, and then creates folders to save results for different methods using the specified language model. The function returns the spent budget in dollars.",
            "location": "\"/media/root/Toshiba XG3/works/graph-of-thoughts/docs/src/examples/doc_merge/doc_merge.py\":649-678",
            "content": "    :param budget: Language model budget for the execution in dollars.\n    :type budget: float\n    :param lm_name: Name of the language model to be used.\n    :type lm_name: str\n    :return: Spent budget in dollars.\n    :rtype: float\n    \"\"\"\n    orig_budget = budget\n    data_path = os.path.join(os.path.dirname(__file__), \"documents.csv\")\n    data = []\n    with open(data_path, \"r\", encoding=\"utf8\") as f:\n        reader = csv.reader(f)\n        next(reader)\n        for row in reader:\n            row[0] = int(row[0])\n            data.append(row)\n    if data_ids is None or len(data_ids) == 0:\n        data_ids = list(range(len(data)))\n    selected_data = [data[i] for i in data_ids]\n    results_dir = os.path.join(os.path.dirname(__file__), \"results\")\n    if not os.path.exists(results_dir):\n        os.makedirs(results_dir)\n    timestamp = datetime.datetime.now().strftime(\"%Y-%m-%d_%H-%M-%S\")\n    extra_info = f\"{lm_name}_{'-'.join([method.__name__ for method in methods])}\"\n    folder_name = f\"{extra_info}_{timestamp}\"\n    results_folder = os.path.join(results_dir, folder_name)"
        },
        {
            "comment": "This code sets up a results folder, saves the configuration file in JSON format, and initializes logging. It then iterates over selected data and methods, keeping track of remaining budget. If the budget becomes zero, it stops execution and logs an error message.",
            "location": "\"/media/root/Toshiba XG3/works/graph-of-thoughts/docs/src/examples/doc_merge/doc_merge.py\":679-711",
            "content": "    os.makedirs(results_folder)\n    config = {\n        \"data\": selected_data,\n        \"methods\": [method.__name__ for method in methods],\n        \"lm\": lm_name,\n        \"budget\": budget,\n    }\n    with open(os.path.join(results_folder, \"config.json\"), \"w\") as f:\n        json.dump(config, f)\n    logging.basicConfig(\n        filename=os.path.join(results_folder, \"log.log\"),\n        filemode=\"w\",\n        format=\"%(name)s - %(levelname)s - %(message)s\",\n        level=logging.DEBUG,\n    )\n    for method in methods:\n        os.makedirs(os.path.join(results_folder, method.__name__))\n    for data in selected_data:\n        logging.info(f\"Running data {data[0]}: {data[1]}\")\n        if budget <= 0.0:\n            logging.error(\n                f\"Budget has been depleted, stopping. Data {data[0]} has not been run.\"\n            )\n            break\n        for method in methods:\n            logging.info(f\"Running method {method.__name__}\")\n            logging.info(f\"Budget left: {budget}\")\n            if budget <= 0.0:\n                logging.error("
        },
        {
            "comment": "This code chunk initializes a language model, creates an operations graph, and sets up an executor for running the method. If the budget is depleted, it will stop execution. The code then attempts to run the executor and logs any exceptions that occur during execution.",
            "location": "\"/media/root/Toshiba XG3/works/graph-of-thoughts/docs/src/examples/doc_merge/doc_merge.py\":712-740",
            "content": "                    f\"Budget has been depleted, stopping. Method {method.__name__} has not been run.\"\n                )\n                break\n            lm = language_models.ChatGPT(\n                os.path.join(\n                    os.path.dirname(__file__),\n                    \"../../graph_of_thoughts/language_models/config.json\",\n                ),\n                model_name=lm_name,\n                cache=True,\n            )\n            operations_graph = method()\n            executor = controller.Controller(\n                lm,\n                operations_graph,\n                DocMergePrompter(),\n                DocMergeParser(),\n                {\n                    \"documents\": [data[2], data[3], data[4], data[5]],\n                    \"parts\": set(),\n                    \"current\": \"\",\n                    \"method\": method.__name__,\n                },\n            )\n            try:\n                executor.run()\n            except Exception as e:\n                logging.error(f\"Exception: {e}\")\n            path = os.path.join("
        },
        {
            "comment": "This code takes input NDAs, combines them, and evaluates the combined result using an LLM (Language Model). The output is scored based on information coverage without repetition. A budget of 30 is set, with sampling from range(0, 50), and approaches io, cot, tot, got, and got2 are used. The code logs the spent budget after running the function \"run\".",
            "location": "\"/media/root/Toshiba XG3/works/graph-of-thoughts/docs/src/examples/doc_merge/doc_merge.py\":741-766",
            "content": "                results_folder,\n                method.__name__,\n                f\"{data[0]}.json\",\n            )\n            for operation in operations_graph.operations:\n                for thought in operation.thoughts:\n                    thought.state[\"parts\"] = list(thought.state[\"parts\"])\n            executor.output_graph(path)\n            budget -= lm.cost\n    return orig_budget - budget\nif __name__ == \"__main__\":\n    \"\"\"\n    Input (x1, x2, x3, x4): Four NDAs\n    Output (y): A new combined NDA\n    Evaluation: According to information coverage without repetition (scored by the LLM)\n    \"\"\"\n    budget = 30\n    samples = [item for item in range(0, 50)]\n    approaches = [io, cot, tot, got, got2]\n    spent = run(samples, approaches, budget, \"chatgpt\")\n    logging.info(f\"Spent {spent} out of {budget} budget.\")"
        }
    ]
}