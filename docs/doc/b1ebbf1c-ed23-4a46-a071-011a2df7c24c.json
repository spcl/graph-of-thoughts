{
    "summary": "The code initializes the LLaMA 2 model for text generation, sets up configurations and tokenizer, creates a pipeline, defines a method to generate responses by querying the model, formats responses into dictionaries, and extracts \"generated_text\" from multiple query response dictionaries.",
    "details": [
        {
            "comment": "The code imports necessary libraries, defines a class Llama2HF as an interface for using LLaMA 2 models through HuggingFace library, and initializes the class with configuration, model name, and caching options.",
            "location": "\"/media/root/Toshiba XG3/works/graph-of-thoughts/docs/src/graph_of_thoughts/language_models/llamachat_hf.py\":0-30",
            "content": "# Copyright (c) 2023 ETH Zurich.\n#                    All rights reserved.\n#\n# Use of this source code is governed by a BSD-style license that can be\n# found in the LICENSE file.\n#\n# main author: Ales Kubicek\nimport os\nimport torch\nfrom typing import List, Dict, Union\nfrom .abstract_language_model import AbstractLanguageModel\nclass Llama2HF(AbstractLanguageModel):\n    \"\"\"\n    An interface to use LLaMA 2 models through the HuggingFace library.\n    \"\"\"\n    def __init__(\n        self, config_path: str = \"\", model_name: str = \"llama7b-hf\", cache: bool = False\n    ) -> None:\n        \"\"\"\n        Initialize an instance of the Llama2HF class with configuration, model details, and caching options.\n        :param config_path: Path to the configuration file. Defaults to an empty string.\n        :type config_path: str\n        :param model_name: Specifies the name of the LLaMA model variant. Defaults to \"llama7b-hf\".\n                           Used to select the correct configuration.\n        :type model_name: str\n        :param cache: Flag to determine whether to cache responses. Defaults to False."
        },
        {
            "comment": "The code initializes a class and sets various attributes such as model_id, prompt and response token costs, temperature, top K sampling, and maximum tokens. It also sets the Transformers library cache environment variable before importing it to avoid conflicts with other caches.",
            "location": "\"/media/root/Toshiba XG3/works/graph-of-thoughts/docs/src/graph_of_thoughts/language_models/llamachat_hf.py\":31-52",
            "content": "        :type cache: bool\n        \"\"\"\n        super().__init__(config_path, model_name, cache)\n        self.config: Dict = self.config[model_name]\n        # Detailed id of the used model.\n        self.model_id: str = self.config[\"model_id\"]\n        # Costs for 1000 tokens.\n        self.prompt_token_cost: float = self.config[\"prompt_token_cost\"]\n        self.response_token_cost: float = self.config[\"response_token_cost\"]\n        # The temperature is defined as the randomness of the model's output.\n        self.temperature: float = self.config[\"temperature\"]\n        # Top K sampling.\n        self.top_k: int = self.config[\"top_k\"]\n        # The maximum number of tokens to generate in the chat completion.\n        self.max_tokens: int = self.config[\"max_tokens\"]\n        # Important: must be done before importing transformers\n        os.environ[\"TRANSFORMERS_CACHE\"] = self.config[\"cache_dir\"]\n        import transformers\n        hf_model_id = f\"meta-llama/{self.model_id}\"\n        model_config = transformers.AutoConfig.from_pretrained(hf_model_id)"
        },
        {
            "comment": "The code initializes an LLaMA model for text generation, loads the tokenizer and model configurations, and creates a text generation pipeline. It also provides a function to query the model with a given input query and can generate multiple responses depending on the provided number of desired responses.",
            "location": "\"/media/root/Toshiba XG3/works/graph-of-thoughts/docs/src/graph_of_thoughts/language_models/llamachat_hf.py\":53-81",
            "content": "        bnb_config = transformers.BitsAndBytesConfig(\n            load_in_4bit=True,\n            bnb_4bit_quant_type=\"nf4\",\n            bnb_4bit_use_double_quant=True,\n            bnb_4bit_compute_dtype=torch.bfloat16,\n        )\n        self.tokenizer = transformers.AutoTokenizer.from_pretrained(hf_model_id)\n        self.model = transformers.AutoModelForCausalLM.from_pretrained(\n            hf_model_id,\n            trust_remote_code=True,\n            config=model_config,\n            quantization_config=bnb_config,\n            device_map=\"auto\",\n        )\n        self.model.eval()\n        torch.no_grad()\n        self.generate_text = transformers.pipeline(\n            model=self.model, tokenizer=self.tokenizer, task=\"text-generation\"\n        )\n    def query(self, query: str, num_responses: int = 1) -> List[Dict]:\n        \"\"\"\n        Query the LLaMA 2 model for responses.\n        :param query: The query to be posed to the language model.\n        :type query: str\n        :param num_responses: Number of desired responses, default is 1."
        },
        {
            "comment": "This code defines a method that generates responses from the LLaMA 2 language model. It first checks if the response is cached, then creates a query with system instructions and input. It generates multiple responses using the `generate_text` function, stores them in a list, and formats them into a response dictionary. Finally, it caches the response if necessary.",
            "location": "\"/media/root/Toshiba XG3/works/graph-of-thoughts/docs/src/graph_of_thoughts/language_models/llamachat_hf.py\":82-106",
            "content": "        :type num_responses: int\n        :return: Response(s) from the LLaMA 2 model.\n        :rtype: List[Dict]\n        \"\"\"\n        if self.cache and query in self.respone_cache:\n            return self.respone_cache[query]\n        sequences = []\n        query = f\"<s><<SYS>>You are a helpful assistant. Always follow the intstructions precisely and output the response exactly in the requested format.<</SYS>>\\n\\n[INST] {query} [/INST]\"\n        for _ in range(num_responses):\n            sequences.extend(\n                self.generate_text(\n                    query,\n                    do_sample=True,\n                    top_k=self.top_k,\n                    num_return_sequences=1,\n                    eos_token_id=self.tokenizer.eos_token_id,\n                    max_length=self.max_tokens,\n                )\n            )\n        response = [\n            {\"generated_text\": sequence[\"generated_text\"][len(query) :].strip()}\n            for sequence in sequences\n        ]\n        if self.cache:\n            self.respone_cache[query] = response"
        },
        {
            "comment": "This function takes a list of query response dictionaries, extracts the \"generated_text\" key from each dictionary and returns a list of those extracted texts.",
            "location": "\"/media/root/Toshiba XG3/works/graph-of-thoughts/docs/src/graph_of_thoughts/language_models/llamachat_hf.py\":107-118",
            "content": "        return response\n    def get_response_texts(self, query_responses: List[Dict]) -> List[str]:\n        \"\"\"\n        Extract the response texts from the query response.\n        :param query_responses: The response list of dictionaries generated from the `query` method.\n        :type query_responses: List[Dict]\n        :return: List of response strings.\n        :rtype: List[str]\n        \"\"\"\n        return [query_response[\"generated_text\"] for query_response in query_responses]"
        }
    ]
}